{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7856ce7-45d3-4455-a52a-b3a67ce14dcb",
   "metadata": {},
   "source": [
    "# DSL Clickstream - Task 2\n",
    "\n",
    "For the Task 2 we'll be using the same data layout we have setup previously. We will need to parse the files and create a pipeline that will generate the four tables from our star schema.\n",
    "\n",
    "This should be straightforward to acomplish. Let's begin by defining some helper variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "e719c4d4-36e7-4637-8789-85d2923a54c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on project:  bill-arki1-25-4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Fetch the working project_id\n",
    "PROJECT_ID = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT_ID[0].strip()\n",
    "os.environ[\"PROJECT_ID\"] = PROJECT_ID\n",
    "\n",
    "# Reuse the Task 1 raw bucket data copy we had previously\n",
    "RAWDATA_BUCKET = f\"clickstream-raw-{PROJECT_ID}\"\n",
    "os.environ[\"RAWDATA_BUCKET\"] = RAWDATA_BUCKET\n",
    "\n",
    "# Deadletter bucket to store errors while loading\n",
    "DEADLETTER_BUCKET = f\"clickstream-deadletter-{PROJECT_ID}\"\n",
    "os.environ[\"DEADLETTER_BUCKET\"] = DEADLETTER_BUCKET\n",
    "\n",
    "# Temporary bucket to store artifacts during pipeline execution and data loading\n",
    "TEMPORARY_BUCKET = f\"clickstream-temp-{PROJECT_ID}\"\n",
    "os.environ[\"TEMPORARY_BUCKET\"] = TEMPORARY_BUCKET\n",
    "\n",
    "# Lake dataset\n",
    "LAKE_DATASET=f\"{PROJECT_ID}:clickstream_lake\"\n",
    "os.environ[\"LAKE_DATASET\"] = LAKE_DATASET\n",
    "\n",
    "# DW dataset\n",
    "# Lake dataset\n",
    "DW_DATASET=f\"{PROJECT_ID}:clickstream_dw\"\n",
    "os.environ[\"DW_DATASET\"] = DW_DATASET\n",
    "\n",
    "# Target pipeline code\n",
    "solution_dir = \"../solutions/task2\"\n",
    "solution_schemas = f\"{solution_dir}/schemas.py\"\n",
    "solution_file = f\"{solution_dir}/batch_pipeline.py\"\n",
    "\n",
    "print(\"Running on project: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "dab0a49d-2e0a-43a8-93af-694903520b0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(solution_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "774a860e-9f36-406c-8d8e-fd23d3eeee78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkin storage for RAW DATA...\n",
      "clickstream-raw-bill-arki1-25-4\n",
      "Checkin storage for DEADLETTER DATA...\n",
      "clickstream-deadletter-bill-arki1-25-4\n",
      "Checkin storage for TEMP DATA...\n",
      "clickstream-temp-bill-arki1-25-4\n",
      "Checkin storage for LAKE DATASET...\n",
      "  tableId   Type    Labels   Time Partitioning   Clustered Fields  \n",
      " --------- ------- -------- ------------------- ------------------ \n",
      "  visits    TABLE                                                  \n",
      "Checkin storage for DW DATASET...\n",
      "      tableId        Type    Labels      Time Partitioning        Clustered Fields    \n",
      " ------------------ ------- -------- -------------------------- --------------------- \n",
      "  addtocart_events   TABLE            MONTH (field: timestamp)   session_key          \n",
      "  pageview_events    TABLE            MONTH (field: timestamp)   session_key          \n",
      "  purchase_events    TABLE            MONTH (field: timestamp)   session_key          \n",
      "  sessions           TABLE                                       session_id, user_id  \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Check if buckets for raw data and errors exists, otherwise create them\n",
    "echo \"Checkin storage for RAW DATA...\"\n",
    "gcloud storage buckets describe gs://${RAWDATA_BUCKET} --format='value(name)' 2>/dev/null \\\n",
    "  || gcloud storage buckets create gs://${RAWDATA_BUCKET}\n",
    "echo \"Checkin storage for DEADLETTER DATA...\"\n",
    "gcloud storage buckets describe gs://${DEADLETTER_BUCKET} --format='value(name)' 2>/dev/null \\\n",
    "  || gcloud storage buckets create gs://${DEADLETTER_BUCKET}\n",
    "echo \"Checkin storage for TEMP DATA...\"\n",
    "gcloud storage buckets describe gs://${TEMPORARY_BUCKET} --format='value(name)' 2>/dev/null \\\n",
    "  || gcloud storage buckets create gs://${TEMPORARY_BUCKET} --soft-delete-policy.retention-duration 0\n",
    "\n",
    "# Check if the datasets exist, otherwise create them\n",
    "echo \"Checkin storage for LAKE DATASET...\"\n",
    "bq ls ${LAKE_DATASET} 2>/dev/null || bq mk ${LAKE_DATASET}\n",
    "echo \"Checkin storage for DW DATASET...\"\n",
    "bq ls ${DW_DATASET} 2>/dev/null || bq mk ${DW_DATASET}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362cc8f8-86b1-4643-9eaa-3403eacbf366",
   "metadata": {},
   "source": [
    "## Creating the pipeline and testing locally\n",
    "\n",
    "Let's create the pipeline code and run it inside the notebook as a first test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "c2765073-0871-4408-962f-3a24b58bdc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../solutions/task2/batch_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {solution_file}\n",
    "#!/usr/bin/env python\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import hashlib\n",
    "import json\n",
    "import typing\n",
    "import datetime\n",
    "import google\n",
    "from google.cloud import bigquery\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, SetupOptions, GoogleCloudOptions, StandardOptions\n",
    "from apache_beam.pvalue import TaggedOutput\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "from apache_beam.io.gcp.bigquery import WriteToBigQuery, BigQueryDisposition\n",
    "\n",
    "# Global settings\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "RAWDATA_BUCKET = os.getenv(\"RAWDATA_BUCKET\", default=f\"clickstream-raw-{PROJECT_ID}\")\n",
    "DEADLETTER_BUCKET = os.getenv(\"RAWDATA_BUCKET\", default=f\"clickstream-deadletter-{PROJECT_ID}\")\n",
    "TEMPORARY_BUCKET = os.getenv(\"TEMPORARY_BUCKET\", default=f\"clickstream-tmp-{PROJECT_ID}\")\n",
    "\n",
    "# Bigquery Schemas\n",
    "RAW = {\n",
    "    \"fields\": [\n",
    "        {\"name\": \"session_id\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"user_id\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"device_type\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"geolocation\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"user_agent\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"events\", \"type\": \"RECORD\", \"mode\": \"REPEATED\",\n",
    "            \"fields\": [\n",
    "                {\"name\": \"event\", \"type\": \"RECORD\", \"mode\": \"NULLABLE\",\n",
    "                     \"fields\": [\n",
    "                        {\"name\": \"event_type\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "                        {\"name\": \"timestamp\", \"type\": \"TIMESTAMP\", \"mode\": \"NULLABLE\"},\n",
    "                        {\"name\": \"details\", \"type\": \"RECORD\", \"mode\": \"NULLABLE\",\n",
    "                             \"fields\": [\n",
    "                                 {\"name\": \"page_url\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "                                 {\"name\": \"referrer_url\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "                                 {\"name\": \"product_id\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "                                 {\"name\": \"product_name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "                                 {\"name\": \"category\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "                                 {\"name\": \"price\", \"type\": \"FLOAT\", \"mode\": \"NULLABLE\"},\n",
    "                                 {\"name\": \"quantity\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "                                 {\"name\": \"order_id\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "                                 {\"name\": \"amount\", \"type\": \"FLOAT\", \"mode\": \"NULLABLE\"},\n",
    "                                 {\"name\": \"currency\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "                                 {\"name\": \"items\", \"type\":\"RECORD\", \"mode\": \"REPEATED\",\n",
    "                                      \"fields\": [\n",
    "                                          {\"name\":\"product_id\", \"type\":\"STRING\", \"mode\": \"NULLABLE\"},\n",
    "                                          {\"name\":\"product_name\", \"type\":\"STRING\", \"mode\": \"NULLABLE\"},\n",
    "                                          {\"name\":\"category\", \"type\":\"STRING\", \"mode\": \"NULLABLE\"},\n",
    "                                          {\"name\":\"price\", \"type\":\"FLOAT\", \"mode\": \"NULLABLE\"},\n",
    "                                          {\"name\":\"quantity\", \"type\":\"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "                                      ],\n",
    "                                 },\n",
    "                             ],\n",
    "                        },\n",
    "                     ],\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "SESSION = {\n",
    "    \"fields\": [\n",
    "        {\"name\": \"session_key\",\"type\": \"STRING\",\"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"session_id\",\"type\": \"STRING\",\"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"user_id\",\"type\": \"STRING\",\"mode\": \"NULLABLE\" },\n",
    "        {\"name\": \"user_agent\",\"type\": \"STRING\",\"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"geolocation\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"device_type\",\"type\": \"STRING\",\"mode\": \"NULLABLE\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "PAGEVIEW = {\n",
    "    \"fields\": [\n",
    "        {\"name\": \"session_key\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"timestamp\", \"type\": \"TIMESTAMP\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"page_url\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"referrer_url\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "ADDTOCART = {\n",
    "    \"fields\": [\n",
    "        {\"name\": \"session_key\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"timestamp\", \"type\": \"TIMESTAMP\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"product_id\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"product_name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"category\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"quantity\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"price\", \"type\": \"FLOAT\", \"mode\": \"NULLABLE\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "PURCHASE = {\n",
    "    \"fields\": [\n",
    "        {\"name\": \"session_key\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"timestamp\", \"type\": \"TIMESTAMP\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"order_id\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"currency\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"amount\", \"type\": \"FLOAT\", \"mode\": \"NULLABLE\"},\n",
    "        {\"name\": \"items\", \"type\": \"RECORD\", \"mode\": \"REPEATED\",\n",
    "            \"fields\": [\n",
    "                {\"name\": \"quantity\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "                {\"name\": \"price\", \"type\": \"FLOAT\", \"mode\": \"NULLABLE\"},\n",
    "                {\"name\": \"category\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "                {\"name\": \"product_name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "                {\"name\": \"product_id\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"}\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "class Options(PipelineOptions):\n",
    "    \"\"\" Options provide custom options for the batch pipeline execution. \"\"\"\n",
    "    @classmethod\n",
    "    def _add_argparse_args(cls, parser):\n",
    "        parser.add_argument(\"--src-bucket\", default=f\"gs://{RAWDATA_BUCKET}\",\n",
    "                            help=\"The data bucket to read from\")\n",
    "        parser.add_argument(\"--temp-bucket\", default=f\"gs://{TEMPORARY_BUCKET}\",\n",
    "                            help=\"The temporary bucket to write artifacts into\")\n",
    "        parser.add_argument(\"--lake-dataset\", default=f\"{PROJECT_ID}:clickstream_lake\",\n",
    "                            help=\"The dataset to be used for the Data Lake\")\n",
    "        parser.add_argument(\"--dw-dataset\", default=f\"{PROJECT_ID}:clickstream_dw\",\n",
    "                            help=\"The dataset to be used for the Data Wharehouse\")\n",
    "        parser.add_argument(\"--deadletter-bucket\", default=f\"gs://{DEADLETTER_BUCKET}\",\n",
    "                            help=\"The bucket where to store unparseable data and the errors\")\n",
    "        parser.add_argument(\"--force-recreate\", action=\"store_true\",\n",
    "                            help=\"Delete the tables before running the Pipeline in order to recreate them (i.e. when cluster columns change)\")\n",
    "\n",
    "def parse_json(line):\n",
    "    return json.loads(line)\n",
    "        \n",
    "def new_session_key(session):\n",
    "    \"\"\"\n",
    "    session_key calculates a unique session key from the session info.\n",
    "\n",
    "    Due to possible data duplication, we are using the tuple (session_id, user_id, device)\n",
    "    as input to SHA256 hash function, and the resulting hex representation is the session_key.\n",
    "    \"\"\"\n",
    "    data = [\n",
    "        session[\"session_id\"],\n",
    "        session[\"user_id\"],\n",
    "        session[\"device_type\"]\n",
    "    ]\n",
    "    return hashlib.sha256(\":\".join(data).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def parse_event(session_key, session_data):\n",
    "    \"\"\"\n",
    "    parse_event will unnest the data from the raw schema into the proper event type schema.\n",
    "    \n",
    "    It returns 'invalid', raw_data in case the event type is not a known one.\n",
    "    \"\"\"\n",
    "    event = session_data[\"event\"]\n",
    "    details = event[\"details\"]\n",
    "    \n",
    "    if event[\"event_type\"] == \"page_view\":\n",
    "        return \"page_view\", {\n",
    "            \"session_key\": session_key,\n",
    "            \"timestamp\": event[\"timestamp\"],\n",
    "            \"page_url\": details[\"page_url\"],\n",
    "            \"referrer_url\": details[\"referrer_url\"]\n",
    "        }\n",
    "    elif event[\"event_type\"] == \"add_item_to_cart\":\n",
    "        return \"add_item_to_cart\", {\n",
    "            \"session_key\": session_key,\n",
    "            \"timestamp\": event[\"timestamp\"],\n",
    "            \"product_id\": details[\"product_id\"],\n",
    "            \"product_name\": details[\"product_name\"],\n",
    "            \"category\": details[\"category\"],\n",
    "            \"price\": details[\"price\"],\n",
    "            \"quantity\": details[\"quantity\"]\n",
    "        }\n",
    "    elif event[\"event_type\"] == \"purchase\":\n",
    "        #TODO(ronoaldo): fix the amount rounding error from the input\n",
    "        return \"purchase\", {\n",
    "            \"session_key\": session_key,\n",
    "            \"timestamp\": event[\"timestamp\"],\n",
    "            \"order_id\": details[\"order_id\"],\n",
    "            \"amount\": details[\"amount\"],\n",
    "            \"currency\": details[\"currency\"],\n",
    "            \"items\": details[\"items\"],\n",
    "        }\n",
    "    else:\n",
    "        \"invalid\", event\n",
    "\n",
    "def parse_sessions(line):\n",
    "    \"\"\"\n",
    "    parse_sessions will parse all the session data received as a JSONL string.\n",
    "    \n",
    "    The sessions will be output into a tagged output called 'sessions'.\n",
    "    Additionally, each event will be output into separeted pcollections, according to the event type.\n",
    "    \n",
    "    Invalid events or unknown event_types will be rejected into the _invalid output pcollection.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert the raw session to JSON and yield the session table row\n",
    "        session = json.loads(line)\n",
    "        session_key = new_session_key(session)\n",
    "        yield {\n",
    "            \"session_key\": session_key,\n",
    "            \"session_id\": session[\"session_id\"],\n",
    "            \"user_id\": session[\"user_id\"],\n",
    "            \"device_type\": session[\"device_type\"],\n",
    "            \"geolocation\": session[\"geolocation\"],\n",
    "            \"user_agent\": session[\"user_agent\"],\n",
    "        }\n",
    "    except Exception as e:\n",
    "        yield TaggedOutput(\"invalid\", f\"error={e}; line={line}\")\n",
    "        # Potentially invalid JSON found, so let's ignore the record entirely and return\n",
    "        return\n",
    "\n",
    "    # For each event, emit the appropriate event by type\n",
    "    for session_data in session[\"events\"]:\n",
    "        try:\n",
    "            event_type, event = parse_event(session_key, session_data)\n",
    "            yield TaggedOutput(event_type, event)\n",
    "        except Exception as e:\n",
    "            yield TaggedOutput(\"invalid\", f\"error={e}, session_data={session_data}\")\n",
    "\n",
    "class RecreateTable(beam.PTransform):\n",
    "    \"\"\"\n",
    "    Custom PTransform to overwrite a Bigquery table.\n",
    "    \n",
    "    Defaults to partition by montly timestamp and no clustering.\n",
    "    \"\"\"\n",
    "    def __init__(self, table=None, schema=None, partition_by=\"timestamp\", partition_type=\"MONTH\", cluster_by=None):\n",
    "        self.table = table\n",
    "        self.schema = schema\n",
    "        \n",
    "        self.bq_params = {}\n",
    "        if partition_by is not None:\n",
    "            self.bq_params[\"timePartitioning\"] = {\"type\": partition_type, \"field\": partition_by}\n",
    "        if cluster_by is not None:\n",
    "            cluster_by = cluster_by if isinstance(cluster_by, (list, tuple)) else [cluster_by]\n",
    "            self.bq_params[\"clustering\"] = {\"fields\": cluster_by}\n",
    "\n",
    "    def default_label(self):\n",
    "        table_name = self.table.split(\".\")[-1]\n",
    "        return str(f\"{self.__class__.__name__}_{table_name}\")\n",
    "\n",
    "    def expand(self, pcoll):\n",
    "        return pcoll | WriteToBigQuery(\n",
    "            table=self.table,\n",
    "            schema=self.schema or \"SCHEMA_AUTODETECT\",\n",
    "            create_disposition=BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=BigQueryDisposition.WRITE_TRUNCATE,\n",
    "            method=\"FILE_LOADS\",\n",
    "            additional_bq_parameters=self.bq_params\n",
    "        )\n",
    "\n",
    "def delete_tables(table_ids=[]):\n",
    "    LOG = logging.getLogger(\"batch_pipeline\")\n",
    "    \n",
    "    client = bigquery.Client()\n",
    "    for table_id in table_ids:\n",
    "        LOG.info(\"Removing %s ...\", table_id)\n",
    "        try:\n",
    "            client.delete_table(table_id)\n",
    "        except google.api_core.exceptions.NotFound:\n",
    "            pass\n",
    "    \n",
    "def run_pipeline(args):\n",
    "    \"\"\"\n",
    "    run_pipeline initializes the pipeline and executes according to the arguments provided.\n",
    "    \"\"\"\n",
    "    LOG = logging.getLogger(\"batch_pipeline\")\n",
    "    LOG.setLevel(logging.DEBUG)\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "    \n",
    "    opts = Options(flags=args)\n",
    "    # Set default temp_bucket when running the job\n",
    "    opts.view_as(GoogleCloudOptions).temp_location = opts.temp_bucket + \"/\" + timestamp + \"/\"\n",
    "    # Required to avoid NameError\n",
    "    # Ref: https://cloud.google.com/dataflow/docs/guides/common-errors#name-error\n",
    "    opts.view_as(SetupOptions).save_main_session = True\n",
    "    pipeline = beam.Pipeline(options=opts)\n",
    "    \n",
    "    # Read from Cloud Storage\n",
    "    dead_letter_folder = f\"{opts.deadletter_bucket}/{timestamp}/\"\n",
    "    LOG.info(f\"Reading data from {opts.src_bucket}; writting errors into {dead_letter_folder}\")\n",
    "    json_lines = pipeline | \"LoadJSONFiles\" >> ReadFromText(opts.src_bucket + \"/*.jsonl\")\n",
    "    \n",
    "    # TODO:(ronoaldo) load the session data as a raw table?\n",
    "    json_lines | \"ParseRawAsJSON\" >> beam.Map(parse_json) | RecreateTable(\n",
    "        table=f\"{opts.lake_dataset}.visits\",\n",
    "        schema=RAW,\n",
    "        cluster_by=None,\n",
    "        partition_by=None)\n",
    "    \n",
    "    # Parse the session data\n",
    "    parsed = json_lines | \"ParseSessionData\" >> beam.ParDo(parse_sessions).with_outputs(\n",
    "        \"page_view\", \"add_item_to_cart\", \"purchase\", \"invalid\", main=\"session\"\n",
    "    )\n",
    "    \n",
    "    # Dead-letter the errors to an error bucket\n",
    "    parsed.invalid | \"InvalidToDeadLetter\" >> WriteToText(dead_letter_folder)\n",
    "    \n",
    "    # Store the sessions\n",
    "    parsed.session | RecreateTable(\n",
    "        table=f\"{opts.dw_dataset}.sessions\",\n",
    "        schema=SESSION,\n",
    "        partition_by=None,\n",
    "        cluster_by=[\"session_id\", \"user_id\"])\n",
    "    \n",
    "    # Store the events\n",
    "    parsed.page_view | RecreateTable(\n",
    "        table=f\"{opts.dw_dataset}.pageview_events\",\n",
    "        schema=PAGEVIEW,\n",
    "        cluster_by=\"session_key\")\n",
    "    \n",
    "    parsed.add_item_to_cart | RecreateTable(\n",
    "        table=f\"{opts.dw_dataset}.addtocart_events\",\n",
    "        schema=ADDTOCART,\n",
    "        cluster_by=\"session_key\")\n",
    "    \n",
    "    parsed.purchase | RecreateTable(\n",
    "        table=f\"{opts.dw_dataset}.purchase_events\",\n",
    "        schema=PURCHASE,\n",
    "        cluster_by=\"session_key\")\n",
    "    \n",
    "    # Patch the table to update the clustering fields\n",
    "    if opts.force_recreate == True:\n",
    "        LOG.info(\"force_recreate: removing removing old versions of tables\")\n",
    "        delete_tables(table_ids=[\n",
    "            f\"{opts.lake_dataset.replace(':','.')}.visits\",\n",
    "            f\"{opts.dw_dataset.replace(':','.')}.sessions\",\n",
    "            f\"{opts.dw_dataset.replace(':','.')}.pageview_events\",\n",
    "            f\"{opts.dw_dataset.replace(':','.')}.addtocart_events\",\n",
    "            f\"{opts.dw_dataset.replace(':','.')}.purchase_events\"])\n",
    "\n",
    "    # Run the pipeline\n",
    "    LOG.info(\"Launching the pipeline ...\")\n",
    "    pipeline.run().wait_until_finish()\n",
    "    LOG.info(\"Execution finished\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "d1e574b8-14f9-4aa9-8c99-376c5b8f3c87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: batch_pipeline.py [-h] [--src-bucket SRC_BUCKET]\n",
      "                         [--temp-bucket TEMP_BUCKET]\n",
      "                         [--lake-dataset LAKE_DATASET]\n",
      "                         [--dw-dataset DW_DATASET]\n",
      "                         [--deadletter-bucket DEADLETTER_BUCKET]\n",
      "                         [--force-recreate]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --src-bucket SRC_BUCKET\n",
      "                        The data bucket to read from\n",
      "  --temp-bucket TEMP_BUCKET\n",
      "                        The temporary bucket to write artifacts into\n",
      "  --lake-dataset LAKE_DATASET\n",
      "                        The dataset to be used for the Data Lake\n",
      "  --dw-dataset DW_DATASET\n",
      "                        The dataset to be used for the Data Wharehouse\n",
      "  --deadletter-bucket DEADLETTER_BUCKET\n",
      "                        The bucket where to store unparseable data and the\n",
      "                        errors\n",
      "  --force-recreate      Delete the tables before running the Pipeline in order\n",
      "                        to recreate them (i.e. when cluster columns change)\n"
     ]
    }
   ],
   "source": [
    "!python3 {solution_file} --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "badfd0de-650e-41ff-a3db-5224967b6a63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:batch_pipeline:Reading data from gs://clickstream-raw-bill-arki1-25-4; writting errors into gs://clickstream-raw-bill-arki1-25-4/2025-04-22_01:02:39/\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.UpdateDestinationSchema'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.TriggerCopyJobs'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.DeleteTablesFn'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.UpdateDestinationSchema'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.TriggerCopyJobs'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.DeleteTablesFn'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.UpdateDestinationSchema'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.TriggerCopyJobs'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.DeleteTablesFn'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.UpdateDestinationSchema'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.TriggerCopyJobs'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.DeleteTablesFn'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.UpdateDestinationSchema'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.TriggerCopyJobs'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.DeleteTablesFn'>)\n",
      "INFO:batch_pipeline:Launching the pipeline ...\n"
     ]
    }
   ],
   "source": [
    "!rm -rf beam-temp-* output-*\n",
    "!python3 {solution_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23708f17-2811-4383-9b70-de96b844af21",
   "metadata": {},
   "source": [
    "## Run on Dataflow\n",
    "\n",
    "Now that the code is properly running, let's submit the JOB to the Dataflow runtime.\n",
    "\n",
    "First we need to create another file for setuptools to identify our packages since we split our code into two files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "30d3e25a-3964-4502-b714-5a660793c0e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:batch_pipeline:Reading data from gs://clickstream-raw-bill-arki1-25-4; writting errors into gs://clickstream-raw-bill-arki1-25-4/2025-04-22_01:06:49/\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.UpdateDestinationSchema'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.TriggerCopyJobs'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.DeleteTablesFn'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.UpdateDestinationSchema'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.TriggerCopyJobs'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.DeleteTablesFn'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.UpdateDestinationSchema'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.TriggerCopyJobs'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.DeleteTablesFn'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.UpdateDestinationSchema'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.TriggerCopyJobs'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.DeleteTablesFn'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.UpdateDestinationSchema'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.TriggerCopyJobs'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.DeleteTablesFn'>)\n",
      "INFO:batch_pipeline:Launching the pipeline ...\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.UpdateDestinationSchema'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.TriggerCopyJobs'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.DeleteTablesFn'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.UpdateDestinationSchema'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.TriggerCopyJobs'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.DeleteTablesFn'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.UpdateDestinationSchema'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.TriggerCopyJobs'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.DeleteTablesFn'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.UpdateDestinationSchema'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.TriggerCopyJobs'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.DeleteTablesFn'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.UpdateDestinationSchema'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.TriggerCopyJobs'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.DeleteTablesFn'>)\n",
      "INFO:batch_pipeline:Execution finished\n"
     ]
    }
   ],
   "source": [
    "!python3 {solution_file} --runner=DataflowRunner --project={PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bb4e2e-8bc3-4e11-9532-55e8347bb3cc",
   "metadata": {},
   "source": [
    "## Possible Improvements\n",
    "\n",
    "During testing we had some issues trying to configure the code to be split into multiple files in Python. This required us to use a few tricks, like setting up the pipeline to always pickle the global state (otherwise, functions defined by the script would not load).\n",
    "\n",
    "An improvement for a future revision would be to split the code into multiple files and use the setup.py file to specify the pipeline as a python Module. Futher improvements would be to allow the pipeline to be packaged and implemented as a redistributable and possibly reusable Python module. This would allow for the code to be properly executed in Dataflow without the name errors we experienced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d915802b-0f09-40fa-a9a4-a355e2b02798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-apache-beam-apache-beam",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Apache Beam (Local)",
   "language": "python",
   "name": "conda-env-apache-beam-apache-beam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
